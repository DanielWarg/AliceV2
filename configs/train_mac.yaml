# ===== Alice DPO (Mac/MPS) - ultrakort =====
model_name: Qwen/Qwen2.5-3B-Instruct   # öppen modell, bra för MPS
output_dir: outputs/alice_dpo_mac_v1

train_file: data/dpo_v1/splits/v1/train.jsonl
val_file:   data/dpo_v1/splits/v1/val.jsonl

# sekvens & batching
max_length: 2048
per_device_train_batch_size: 1
per_device_eval_batch_size: 1
gradient_accumulation_steps: 16

# opt & schema
learning_rate: 5.0e-6
weight_decay: 0.0
warmup_ratio: 0.06
beta: 0.2                 # 0.1–0.3 är rimligt
loss_type: ipo            # viktig för MPS-minne (referensfri DPO)

# körning
max_steps: 2000           # eller num_train_epochs: 1
logging_steps: 25
eval_steps: 100
save_steps: 200
save_total_limit: 2
seed: 42
report_to: "none"

# LoRA (16-bit på MPS)
lora_r: 16
lora_alpha: 32
lora_dropout: 0.05
target_modules: [q_proj, k_proj, v_proj, o_proj, gate_proj, up_proj, down_proj]

# tokenisering/chat
use_chat_template: true   # wrappar prompten i modellens chat-template
eos_token_fallback: true  # sätter pad_token=eos om saknas

# MPS (fp32 - fp16 ej stödd på MPS)
fp16: false
bf16: false
gradient_checkpointing: true
use_cache: false