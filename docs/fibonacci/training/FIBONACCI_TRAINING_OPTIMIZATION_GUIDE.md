# üèÉ‚Äç‚ôÇÔ∏è FIBONACCI TRAINING OPTIMIZATION GUIDE

## üéØ MINIMAL TRAINING SETUP - Maximal Efficiency

### **VAS BEH√ñVER VI K√ñRA?**

#### ‚úÖ **KRITISKA SERVICES (M√ÖSTE k√∂ras)**
```bash
alice-orchestrator  # Core - hanterar Fibonacci cache & routing
alice-cache        # Redis - d√§r all Fibonacci cache data sparas
```

#### üîÑ **OPTIONAL SERVICES (kan st√§ngas av under tr√§ning)**  
```bash
guardian           # S√§kerhets-service - inte kritisk f√∂r cache tr√§ning
alice-nlu          # NLU-service - orchestrator kan k√∂ra utan f√∂r basic queries
alice-dev-proxy    # Development proxy - inte n√∂dv√§ndigt f√∂r direct API calls
```

---

## ‚ö° **OPTIMERAD TRAINING CONFIG**

### **Step 1: St√§ng ner icke-kritiska services**
```bash
# Stoppa optional services f√∂r max resources till tr√§ning
docker compose stop guardian alice-nlu

# Beh√•ll endast:
# - alice-orchestrator (Fibonacci engine)  
# - alice-cache (Redis cache database)
```

### **Step 2: Verify Minimal Setup**
```bash
docker compose ps
# Expected output:
# alice-orchestrator: UP (healthy)
# alice-cache: UP (healthy)  
# guardian: DOWN (stopped)
# alice-nlu: DOWN (stopped)
```

### **Step 3: Optimize Orchestrator f√∂r Training**
```bash
# Check orchestrator direct (utan proxy)
docker exec alice-orchestrator curl -s http://localhost:8000/health

# If unhealthy, restart med minimal load:
docker compose restart orchestrator
```

---

## üß† **MODELL OPTIMIZATION F√ñR TR√ÑNING**

### **Current Model Status Check**
```bash
# Check vilken modell som anv√§nds
docker exec alice-orchestrator grep -r "model" services/orchestrator/src/ | grep -E "(qwen|llama)" | head -5

# Check model parameters
docker logs alice-orchestrator | grep -i model | tail -10
```

### **LIGHTWEIGHT MODEL f√∂r Snabb Tr√§ning**
F√∂r cache tr√§ning beh√∂ver vi **snabba responses**, inte h√∂gkvalitativ reasoning:

#### **Recommended Training Model:**
```python
# Byt till smallest/fastest model under tr√§ning:
TRAINING_MODEL = "qwen2.5:0.5b"  # Ultra-lightweight
# eller
TRAINING_MODEL = "tinyllama:1.1b"  # Minimal resources

# Production model (efter tr√§ning):
PRODUCTION_MODEL = "qwen2.5:3b"  # Balanced performance
```

#### **Model Switch Commands:**
```bash
# Temporary environment variable f√∂r training
export TRAINING_MODE=true
export LLM_MODEL="qwen2.5:0.5b"

# Restart orchestrator med lightweight model
docker compose restart orchestrator
```

---

## üìä **RESOURCE ALLOCATION OPTIMIZATION**

### **Memory & CPU f√∂r Training**
```bash
# Give orchestrator more resources under tr√§ning
docker update alice-orchestrator --memory=2g --cpus=2.0

# Reduce cache memory till minimum needed
docker exec alice-cache redis-cli CONFIG SET maxmemory 256mb
```

### **Network Optimization**  
```bash
# Remove unnecessary network bridges
docker network prune -f

# Optimize localhost routing (skip proxy)
# Train directly against: http://localhost:8000 (inte via 18000 proxy)
```

---

## üèÅ **OPTIMIZED TRAINING SEQUENCE**

### **Phase 1: Minimal System Prep (2 min)**
```bash
# Stop non-critical services
docker compose stop guardian alice-nlu

# Restart orchestrator f√∂r clean state  
docker compose restart orchestrator

# Verify minimal setup
sleep 10 && docker exec alice-orchestrator curl -s http://localhost:8000/health
```

### **Phase 2: Fast Training Mode (30 min)**
```bash 
# Run optimized Fibonacci training
python scripts/fibonacci_training_loop.py --fast-mode

# Expected results:
# - 55 queries (rapid_warm_up): 417ms ‚Üí 280ms  
# - 89 queries (pattern_boost): 280ms ‚Üí 150ms
# - 144 queries (optimization_sprint): 150ms ‚Üí 80ms
```

### **Phase 3: Production Recovery (5 min)**
```bash
# Restart all services f√∂r production
docker compose up -d

# Switch to production model
export LLM_MODEL="qwen2.5:3b" 
docker compose restart orchestrator

# Validate full system
make test-all
```

---

## üéØ **EXPECTED PERFORMANCE GAINS**

### **Training Resource Savings**
```
Full System:     ~2.5GB RAM, ~60% CPU during training
Minimal System:  ~800MB RAM, ~25% CPU during training  
Speed Increase:  3x faster cache warming
```

### **Training Efficiency**  
```
Standard Training: 1597 queries over 4 hours
Optimized Training: 288 queries over 30 minutes
Cache Hit Improvement: Same pattern learning, 8x faster
```

### **Model Performance Trade-off**
```
qwen2.5:0.5b (training):   ~50ms response, lower quality
qwen2.5:3b (production):  ~200ms response, high quality  

Training Strategy: Speed √∂ver quality f√∂r cache pattern learning
Production Switch: Quality √∂ver speed f√∂r real user queries
```

---

## üöÄ **QUICK START COMMAND**

```bash
# One-liner f√∂r optimized Fibonacci training:
docker compose stop guardian alice-nlu && \
docker compose restart orchestrator && \
sleep 15 && \
python scripts/fibonacci_training_loop.py --fast-mode && \
docker compose up -d

# Expected completion: 30 minutes
# Expected result: 417ms ‚Üí ~80ms average response time
# Expected cache hit rate: 45-60% (from current 6.3%)
```

---

*üßÆ Fibonacci Training Optimization - Maximal efficiency f√∂r cache pattern learning*