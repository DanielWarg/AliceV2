# Alice v2 - Daily Status & Next Steps

_Uppdaterad: 2025-09-01 - Eftermiddagsrapport_

## üéØ **DAGENS SLUTSTATUS**

### ‚úÖ **VAD VI HAR GJORT IDAG**

Kort: Docker-only dev-proxy, Observability+HUD klart, `/metrics` exponerad, NLU-service scaffoldad, curator/scheduler pipeline p√• plats (scheduler optional), Orchestrator‚ÜîGuardian URL fix, NLU-proxy routing, auto_verify uppdaterad.

1. **ü§ñ LLM Drivers Implementerade:**
   - `services/orchestrator/src/llm/ollama_client.py` - Ollama client med timeouts
   - `services/orchestrator/src/llm/micro_phi.py` - Phi-mini driver f√∂r snabba svar
   - `services/orchestrator/src/llm/planner_qwen.py` - Qwen-MoE driver med JSON output
   - `services/orchestrator/src/llm/deep_llama.py` - Llama-3.1 driver med Guardian integration

2. **üß† Intelligent Routing:**
   - `services/orchestrator/src/router/policy.py` - Pattern matching f√∂r micro/planner/deep
   - Testat och fungerar: "Hej Alice" ‚Üí micro, "Boka m√∂te" ‚Üí planner, "Analysera" ‚Üí deep

3. **üìã Planner Execution:**
   - `services/orchestrator/src/planner/schema.py` - JSON schema f√∂r tool execution
   - `services/orchestrator/src/planner/execute.py` - Plan execution med fallback matrix

4. **üõ°Ô∏è Guardian Integration:**
   - Deep model blocked under brownout
   - Planner degraded under resurstryck
   - Micro always available

5. **üê≥ Docker-only + Dev-proxy (18000):**
   - `ops/Caddyfile` - Proxy till orchestrator/guardian/HUD/NLU
   - `scripts/dev_up.sh`/`dev_down.sh` - Deterministisk start/stop
   - Compose: inga host-portar utom proxy; healthchecks p√• alla services

6. **üìà Observability & Metrics:**
   - `/metrics` endpoint i Orchestrator (Prometheus-format)
   - P50/P95 per route via middleware; `/api/chat` s√§tter `X-Route` tidigt
   - Turn events (JSONL) med RAM-peak, energy Wh, input/output/lang under `data/telemetry/YYYY-MM-DD/`

7. **üß† NLU-service (svenska) ‚Äì v1 scaffold:**
   - `services/nlu/` (FastAPI) med `/api/nlu/parse`
   - Baseline intents/slots (regex + dateparser), proxy routing via `/api/nlu/*`
   - Orchestrator anropar NLU och s√§tter `X-Intent`/`X-Route-Hint`

8. **ü™Ñ Dataloop (curator):**
   - `services/curator/` + `ops/schedule.cron` (indexer commented)
   - `scripts/auto_verify.sh` k√∂r curator och sparar summary

9. **üß† NLU v1 (svenska):**
   - `/api/nlu/parse` aktiv; e5-embeddings + heuristik
   - Orchestrator `/api/chat` s√§tter `X-Intent`/`X-Route-Hint` och `X-Route`
   - Eval-svit ut√∂kad till 20 scenarier (micro/planner) ‚Äì 100% pass
   - Cron installerad: auto_verify kl 14:00 ‚Üí `logs/auto_verify.log`

## üö® **AKTUELLT PROBLEM**

Alla tidigare portkrockar eliminerade med Docker-only + dev-proxy. Scheduler-bild √§r optional (kan l√§mnas av tillsvidare). Ollama tillagd i compose; modell-pull och val av mikro-modell sker i n√§sta steg.

### **Kvarst√•ende sm√•saker:**

1. NLU ONNX (multilingual-e5-small) + XNLI (int8) och threshold-tuning
2. Micro-LLM (Phi-mini) via Ollama (modell-pull och sanity)
3. Scheduler-image (cron) kan bytas till publik om nattk√∂rningar √∂nskas i dev

## üöÄ **N√ÑSTA STEG (IMORGON)**

### **Steg 1: Start & Sanity**

```bash
scripts/dev_up.sh
curl -s http://localhost:18000/health | jq .
curl -s http://localhost:18000/api/status/routes | jq .
```

### **Steg 2: NLU + Mikro-LLM**

```bash
# NLU sanity
curl -s -X POST http://localhost:18000/api/nlu/parse -H 'Content-Type: application/json' \
  -d '{"v":"1","lang":"sv","text":"Boka m√∂te med Anna imorgon kl 14","session_id":"nlu-sanity"}' | jq .

# (Efter modell-pull) mikro-LLM sanity
curl -s -X POST http://localhost:18000/api/orchestrator/chat -H 'Content-Type: application/json' \
  -d '{"v":"1","session_id":"dev","lang":"sv","message":"Hej Alice, vad √§r klockan?"}' | jq .
```

### **Steg 3: Validera System**

```bash
./scripts/auto_verify.sh
cat data/tests/summary.json | jq .
open http://localhost:18000/hud
```

### **Steg 4: N√§sta Prioritet**

Om LLM integration fungerar ‚Üí **Voice Pipeline Implementation**

- ASR‚ÜíNLU‚ÜíTTS pipeline med WebSocket connections
- Svenska spr√•kmodeller (Whisper ASR)
- Ut√∂ka `services/eval/scenarios.json` med voice scenarios

## üìã **ACCEPTANSKRITERIER**

### **LLM Integration v1 √§r klar n√§r:**

- [ ] **Routing fungerar**: micro/planner/deep routes v√§ljs korrekt
- [ ] **LLM drivers fungerar**: Ollama integration med proper timeouts
- [ ] **Planner execution**: JSON schema validation + tool execution
- [ ] **Guardian integration**: Deep blocked i brownout, planner degraded
- [ ] **Fallback matrix**: LLM + tool fallback med max 1 kedja per turn
- [ ] **SLO compliance**: Fast ‚â§250ms, planner ‚â§1500ms, deep ‚â§3000ms
- [ ] **Eval harness**: 20 scenarier passerar med ‚â•80% success rate

## üîß **TEKNISKA DETALJER**

### **Branch:**

- `feat/llm-integration-v1` - Alla √§ndringar committade

### **Nyckelfiler:**

- `services/orchestrator/src/llm/` - LLM drivers
- `services/orchestrator/src/router/` - Intelligent routing
- `services/orchestrator/src/planner/` - Planner execution
- `scripts/ports-kill.sh` - Port cleanup
- `scripts/start-llm-test.sh` - LLM test script

### **Milj√∂variabler:**

```bash
OLLAMA_HOST=http://ollama:11434
LLM_MICRO=phi3.5:mini
LLM_PLANNER=qwen2.5:7b-moe
LLM_DEEP=llama3.1:8b
LLM_TIMEOUT_MS=1800
```

## üéØ **SUCCESS METRICS**

### **Tekniska:**

- Routing accuracy: ‚â•90% korrekt route selection
- LLM response time: Inom SLO (‚â§250ms, ‚â§1500ms, ‚â§3000ms)
- Guardian integration: Deep blocked under brownout
- Fallback success: ‚â•80% fallback success rate

### **Business:**

- E2E test success: ‚â•80% pass rate
- System stability: Inga crashes under normal load
- Observability: Alla metrics loggade korrekt

---

**N√ÑSTA AI AGENT**: Du har allt du beh√∂ver! Starta med `./scripts/ports-kill.sh` och `./scripts/start-llm-test.sh`, testa LLM integrationen, och validera att allt fungerar enligt acceptanskriterierna ovan.
